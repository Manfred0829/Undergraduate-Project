from app.utils import media_processer as media, text_processer as text, similarity_calculator as sim
from app.services import file_service
from app.modules.models_request.OCRspace_request import OCRspaceRequest
from app.modules.models_local.EasyOCR_local import EasyOCRLocal
from app.modules.models_request.OpenAI_request import OpenAIRequest
from app.modules.questioning_manager import QuestioningManager
import os
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def ensure_directory_exists(directory_path):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºå®ƒ"""
    if not os.path.exists(directory_path):
        os.makedirs(directory_path, exist_ok=True)
        logger.info(f"å‰µå»ºç›®éŒ„: {directory_path}")

def processing_note(subject, lecture_name, img_path):
    """
    è™•ç†ä¸Šå‚³çš„ç­†è¨˜åœ–ç‰‡
    
    Args:
        subject: ç§‘ç›®åç¨±
        img_path: åœ–ç‰‡è·¯å¾‘
    
    Returns:
        dict: è™•ç†çµæœ
    """
    # å¾ img_path ä¸­æå–åŸå§‹æª”æ¡ˆåç¨±
    original_filename = os.path.basename(img_path)
    filename_without_ext = os.path.splitext(original_filename)[0]

    try:
        logger.info(f"ğŸ” é–‹å§‹è™•ç†ç­†è¨˜: {img_path}, ç§‘ç›®: {subject}")

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        notes_output_dir = os.path.join("app", "data_server", subject, "notes")
        ensure_directory_exists(notes_output_dir)
        
        # Img process
        EasyOCR = EasyOCRLocal()
        img_PIL = media.read_image_to_PIL(img_path)
        if img_PIL is None:
            logger.error(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡: {img_path}")
            return {"success": False, "error": "ç„¡æ³•è®€å–åœ–ç‰‡æª”æ¡ˆ"}
        
        save_path = os.path.join("app", "data_server", subject, "notes", filename_without_ext + "_lines_bounding_box.png")
        cropped_images = EasyOCR.processing_lines_bounding_box(img_PIL, draw_result=True, save_path=save_path)
        logger.info(f"âœ… å·²è™•ç†åœ–ç‰‡ä¸¦åˆ†å‰²æˆ {len(cropped_images)} å€‹æ–‡å­—å€åŸŸ")

        # OCR process
        OpenAI = OpenAIRequest()

        page_texts = ""
        for cropped_img in cropped_images:
            base64_img = media.convert_PIL_to_base64(cropped_img)
            OCR_result_text = OpenAI.generate_img_OCR(base64_img)
            #logger.info(f"OCRè­˜åˆ¥çµæœ: {OCR_result_text[:30]}...")
            page_texts += OCR_result_text + "\n"

        # repair process
        repaired_page = OpenAI.processing_notes_repair(page_texts)
        logger.info("âœ… ç­†è¨˜ä¿®å¾©å®Œæˆ")
        
        # extract process
        notes_json = OpenAI.processing_notes_extract_keypoints(repaired_page)
        logger.info("âœ… é‡é»æå–å®Œæˆ")

        # correct process
        for note in notes_json:
            result = OpenAI.processing_notes_correct(subject,note)
            note['isCorrected'] = result['isCorrected']

            # å¦‚æœç­†è¨˜è§€å¿µéŒ¯èª¤å‰‡å°‡Contentæ›¿æ›æˆæ›´æ­£å¾Œï¼Œä¸¦å¦å¤–ä¿ç•™åŸå§‹å…§å®¹
            if result['isCorrected']:
                note['Wrong_Content'] = note['Content']
                note['Content'] = result['Corrected_Content']
        logger.info("âœ… é‡é»è§€å¿µä¿®æ­£å®Œæˆ")


        # embedding process
        texts_for_embedding = [nt["Title"] + ":\n" + nt["Content"] for nt in notes_json]
        vectors = OpenAI.processing_embedding(texts_for_embedding)
        for i in range(len(vectors)):
            notes_json[i]['Embedding'] = vectors[i]
        logger.info("âœ… åµŒå…¥å‘é‡è™•ç†å®Œæˆ")


        # simularity process
        lecturename_without_ext = os.path.splitext(lecture_name)[0]
        keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json") # è®€å–keypointsè³‡æ–™
        keypoints_json = text.read_json(keypoints_path)
        keypoints_embedding = [k["Embedding"] for k in keypoints_json]

        def _calculate_learning_rate(progress,diff):
            if progress <= 0: # è‹¥å°æ–¼0å‰‡è¨­ç‚º0%
                return 0.0
            elif progress >= 3*diff: # è‹¥å¤§æ–¼ä¸Šé™å‰‡ç‚º100%
                return 1.0
            else:
                return float(progress / (3*diff))

        for n_idx, note in enumerate(notes_json):
            # ç­†è¨˜jsonä¸­å„²å­˜k_idx
            k_idx = sim.get_most_similar_index(note["Embedding"],keypoints_embedding)
            #print(f"k_idx: {k_idx}")
            note["Keypoint_Index"] = -1
            note["Keypoint_Index"] = k_idx

            # è¬›ç¾©jsonä¸­å„²å­˜n_idx
            note_info = {"Notes_File_Name":original_filename , "Note_Index":n_idx}
            if "Notes" not in keypoints_json[k_idx]: # ç¬¬ä¸€æ¬¡åŠ å…¥ç­†è¨˜
                keypoints_json[k_idx]["Notes"] = [note_info]
            else:
                keypoints_json[k_idx]["Notes"].append(note_info)

            if note['isCorrected']: # ç­†è¨˜éŒ¯èª¤
                keypoints_json[k_idx]['Learning_Progress'] -= 2
                keypoints_json[k_idx]['Learning_Rate'] = _calculate_learning_rate(keypoints_json[k_idx]['Learning_Progress'],keypoints_json[k_idx]['Difficulty'])
            else: # ç­†è¨˜æ­£ç¢º
                keypoints_json[k_idx]['Learning_Progress'] += 1
                keypoints_json[k_idx]['Learning_Rate'] = _calculate_learning_rate(keypoints_json[k_idx]['Learning_Progress'],keypoints_json[k_idx]['Difficulty'])
        
        text.write_json(keypoints_json,keypoints_path) # å„²å­˜æ›´æ–°å¾Œçš„keypointsè³‡æ–™
        logger.info("âœ… ç›¸ä¼¼åº¦å°æ‡‰è™•ç†å®Œæˆ")
        
        # save
        #print(f"notes_json: {notes_json}")
        notes_save = {"Lecture_Name": lecture_name, "Notes": notes_json}
        output_path = os.path.join(notes_output_dir, f"{filename_without_ext}.json")
        text.write_json(notes_save, output_path)

        # index.json æ›´æ–°
        file_service.update_file_status(subject, "notes", original_filename, "done")

        logger.info(f"âœ… ç­†è¨˜è™•ç†å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_path}")
        return {"success": True, "message": "ç­†è¨˜è™•ç†å®Œæˆ", "output_path": output_path}
        
    except Exception as e:
        try:
            # index.json æ›´æ–°
            file_service.update_file_status(subject, "notes", original_filename, "error")
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° index.json æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)

        logger.error(f"âŒ è™•ç†ç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        return {"success": False, "error": str(e)}


def processing_lecture(subject, pdf_path):
    """
    è™•ç†ä¸Šå‚³çš„è¬›ç¾©PDF
    
    Args:
        subject: ç§‘ç›®åç¨±
        pdf_path: PDFè·¯å¾‘
    
    Returns:
        dict: è™•ç†çµæœ
    """
    original_filename = os.path.basename(pdf_path)
    filename_without_ext = os.path.splitext(original_filename)[0]

    try:
        logger.info(f"ğŸ” é–‹å§‹è™•ç†è¬›ç¾©: {pdf_path}, ç§‘ç›®: {subject}")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        lectures_output_dir = os.path.join("app", "data_server", subject, "lectures")
        ensure_directory_exists(lectures_output_dir)

        # åƒ…ç”¨æ–¼æ¸¬è©¦ - è™•ç†å‰8é 
        #OCR_result_list = text.read_json(pdf_path)
        #pages_list = pages_list[0:min(8, len(pages_list))]

        # OCR procrss
        img_list = media.read_pdf_to_images(pdf_path)
        if not img_list or len(img_list) == 0:
            logger.error(f"âŒ ç„¡æ³•è®€å–PDF: {pdf_path}")
            return {"success": False, "error": "ç„¡æ³•è®€å–PDFæª”æ¡ˆ"}
            
        OCRspace = OCRspaceRequest()
        
        # é™åˆ¶è™•ç†é æ•¸ï¼Œé˜²æ­¢APIæˆæœ¬éé«˜ (æ¸¬è©¦ç”¨)
        max_pages = min(120, len(img_list))
        pages_list = OCRspace.processing_handouts_OCR(img_list,language='cht')
        logger.info(f"âœ… OCRè­˜åˆ¥å®Œæˆï¼Œå…±è™•ç† {len(pages_list)} é ")
    
        # extract keypoints process
        OpenAI = OpenAIRequest()

        pages_json = []
        for i, page_text in enumerate(pages_list):
            page_json = {'Original_text': page_text}
            page_json['Keypoints'] = OpenAI.processing_handouts_extract_keypoints(page_text)
            page_json['Info'] = OpenAI.processing_handouts_page_info(page_text)
            page_json['page_idx'] = i
            pages_json.append(page_json)

        logger.info(f"âœ… å·²è™•ç† {len(pages_json)} é ä¸¦æå–é‡é»")

        # extract topic process
        topics_json = OpenAI.processing_handouts_extract_topic([page_json['Info'] for page_json in pages_json])
        logger.info(f"âœ… å·²æå– {len(topics_json)} å€‹ä¸»é¡Œ")

        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹ä¸»é¡Œ
        if not topics_json:
            logger.warning("â— æ²’æœ‰æ‰¾åˆ°ä»»ä½•ä¸»é¡Œï¼Œå°‡å»ºç«‹é è¨­ä¸»é¡Œ")
            topics_json = [{
                "Topic": "é è¨­ä¸»é¡Œ",
                "Starting_page": 1
            }]

        # ç‚ºæ¯å€‹ä¸»é¡Œé—œè¯ç›¸æ‡‰çš„é é¢
        for i, topic in enumerate(topics_json):
            start_page = max(0, topic['Starting_page'] - 1)  # ç¢ºä¿ç´¢å¼•è‡³å°‘ç‚º0
            end_page = topics_json[i + 1]['Starting_page'] - 1 if i + 1 < len(topics_json) else len(pages_json)
            # åŠ å…¥ 'Pages' æ¬„ä½ï¼Œå…§å« pages_json ä¸­å°æ‡‰çš„æ¯ä¸€é è³‡æ–™
            topic['Pages'] = pages_json[start_page:end_page]

        # ç§»é™¤æ²’æœ‰é‡é»çš„Topic
        for i, topic in enumerate(topics_json):
            k_count = 0
            for page in topic['Pages']:
                k_count += len(page['Keypoints'])
            if k_count == 0:
                topics_json.pop(i)

        # extract sections process
        sections_json = OpenAI.processing_handouts_extract_section([topic['Topic'] for topic in topics_json])
        logger.info(f"âœ… å·²æå– {len(sections_json)} å€‹ç« ç¯€")

        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ®µè½
        if not sections_json:
            logger.warning("â— æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ®µè½ï¼Œå°‡å»ºç«‹é è¨­æ®µè½")
            sections_json = [{
                "Section": "é è¨­æ®µè½",
                "Starting_topic": 1
            }]

        # ç‚ºæ¯å€‹æ®µè½é—œè¯ç›¸æ‡‰çš„ä¸»é¡Œ
        for i, section in enumerate(sections_json):
            start_topic = max(0, section['Starting_topic'] - 1)  # ç¢ºä¿ç´¢å¼•è‡³å°‘ç‚º0
            end_topic = sections_json[i + 1]['Starting_topic'] - 1 if i + 1 < len(sections_json) else len(topics_json)
            # åŠ å…¥ 'Topics' æ¬„ä½ï¼Œå…§å« topics_json ä¸­å°æ‡‰çš„æ¯ä¸€é è³‡æ–™
            section['Topics'] = topics_json[start_topic:end_topic]

        # extract chapter process
        try:
            # å˜—è©¦ç²å–ç¬¬ä¸€é çš„æ–‡æœ¬ç”¨æ–¼æå–ç« ç¯€ä¿¡æ¯
            first_page_text = ""
            if sections_json and sections_json[0].get('Topics') and sections_json[0]['Topics'] and \
               sections_json[0]['Topics'][0].get('Pages') and sections_json[0]['Topics'][0]['Pages'] and \
               sections_json[0]['Topics'][0]['Pages'][0].get('Original_text'):
                first_page_text = sections_json[0]['Topics'][0]['Pages'][0]['Original_text']
            else:
                # å¦‚æœç„¡æ³•é€šéé æœŸè·¯å¾‘ç²å–ï¼Œå‰‡å˜—è©¦å¾ pages_json ç²å–ç¬¬ä¸€é æ–‡æœ¬
                if pages_json and pages_json[0].get('Original_text'):
                    first_page_text = pages_json[0]['Original_text']
                    logger.warning("â— ç„¡æ³•å¾ç« ç¯€å±¤æ¬¡ç²å–ç¬¬ä¸€é æ–‡æœ¬ï¼Œä½¿ç”¨ç›´æ¥çš„ç¬¬ä¸€é æ–‡æœ¬ä»£æ›¿")
            
            # ç”Ÿæˆç« ç¯€ä¿¡æ¯
            chapter_json = OpenAI.processing_handouts_extract_chapter(filename_without_ext, first_page_text)
            logger.info("âœ… å·²æå–ç« ç¯€ä¿¡æ¯")

        except Exception as e:
            # å¦‚æœæå–ç« ç¯€éç¨‹ä¸­å‡ºéŒ¯ï¼Œå»ºç«‹é»˜èªç« ç¯€ä¿¡æ¯
            logger.warning(f"â— æå–ç« ç¯€ä¿¡æ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œä½¿ç”¨é»˜èªç« ç¯€ä¿¡æ¯")
            chapter_json = {"Chapter": filename_without_ext or "æœªå‘½åç« ç¯€"}

        # å°‡ç« ç¯€å’Œæ®µè½ä¿¡æ¯é—œè¯èµ·ä¾†
        chapter_json['Sections'] = sections_json

        # ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
        path = os.path.join(lectures_output_dir, filename_without_ext + ".json")
        text.write_json(chapter_json, path)
        logger.info(f"âœ… è¬›ç¾©çµæ§‹å·²ä¿å­˜åˆ°: {path}")

        # embedding process
        keypoints_list = _extract_keypoints_hierarchy(chapter_json)
        
        # ç¢ºä¿æ¯å€‹é—œéµé»æœ‰æœ‰æ•ˆçš„æ¨™é¡Œå’Œå…§å®¹
        for kp in keypoints_list:
            if not kp.get("Title"):
                kp["Title"] = "æœªå®šç¾©æ¨™é¡Œ"
            if not kp.get("Content"):
                kp["Content"] = "æœªå®šç¾©å…§å®¹"
        
        # æ ¼å¼åŒ–é—œéµé»ç”¨æ–¼åµŒå…¥
        keypoints_flatten = [kp["Title"] + ":\n" + kp["Content"] for kp in keypoints_list]
        
        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨é—œéµé»
        if not keypoints_flatten:
            logger.warning("â— æ²’æœ‰æ‰¾åˆ°ä»»ä½•é—œéµé»ï¼Œå°‡ä½¿ç”¨é è¨­å€¼")
            keypoints_flatten = ["æœ¬æ–‡ä»¶æ²’æœ‰æå–åˆ°æœ‰æ•ˆé—œéµé»"]
            # å‰µå»ºä¸€å€‹å…·æœ‰é è¨­å€¼çš„é—œéµé»
            keypoints_list = [{
                "Title": "æœªå®šç¾©æ¨™é¡Œ",
                "Content": "æœ¬æ–‡ä»¶æ²’æœ‰æå–åˆ°æœ‰æ•ˆé—œéµé»",
                "Index": [0, 0, 0, 0],
                "from_page": 1
            }]
        
        vectors = OpenAI.processing_embedding(keypoints_flatten)
        
        # ç¢ºä¿å‘é‡å’Œé—œéµé»åˆ—è¡¨é•·åº¦åŒ¹é…
        if len(vectors) != len(keypoints_list):
            logger.warning(f"â— å‘é‡æ•¸é‡({len(vectors)})èˆ‡é—œéµé»æ•¸é‡({len(keypoints_list)})ä¸åŒ¹é…ï¼Œä½¿ç”¨é›¶å‘é‡è£œé½Š")
            # å¦‚æœå‘é‡å°‘æ–¼é—œéµé»ï¼Œè£œå……é›¶å‘é‡
            while len(vectors) < len(keypoints_list):
                vectors.append([0.0] * 1536)  # text-embedding-3-smallçš„å‘é‡ç¶­åº¦æ˜¯1536
            # å¦‚æœå‘é‡å¤šæ–¼é—œéµé»ï¼Œæˆªæ–·å‘é‡
            vectors = vectors[:len(keypoints_list)]

        # generate difficulity, importance process
        weights = OpenAI.processing_handouts_weights(subject, keypoints_flatten)

        # åŠ å…¥info

        for i, kp in enumerate(keypoints_list):
            kp["Embedding"] = vectors[i]
            kp["Difficulty"] = weights[i]["Difficulty"]
            kp["Importance"] = weights[i]["Importance"]
            kp["Learning_Progress"] = 0
            kp["Learning_Rate"] = 0.0

        # save
        keypoints_path = os.path.join(lectures_output_dir, filename_without_ext + "_keypoints.json")
        text.write_json(keypoints_list, keypoints_path)
        logger.info(f"âœ… é‡é»åµŒå…¥å‘é‡å·²ä¿å­˜åˆ°: {keypoints_path}")
        
        # tree diagram process
        tree_path = os.path.join(lectures_output_dir, filename_without_ext + "_tree")
        tree_img_path = tree_path + ".png"
        try:
            media.generate_chapter_hierarchy_graph(chapter_json, tree_path)
            logger.info(f"âœ… æ¨¹ç‹€çµæ§‹åœ–å·²ç”Ÿæˆä¸¦ä¿å­˜åˆ°: {tree_img_path}")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¨¹ç‹€çµæ§‹åœ–æ™‚å‡ºéŒ¯: {e}")
            # ç¢ºä¿å³ä½¿åœ–ç„¡æ³•ç”Ÿæˆï¼Œæ•´å€‹éç¨‹ä¹Ÿèƒ½ç¹¼çºŒ
        

        # topics_json
        topics_json = _extract_topics_hierarchy(chapter_json)
        topics_path = os.path.join(lectures_output_dir, filename_without_ext + "_topics.json")
        text.write_json(topics_json, topics_path)
        logger.info(f"âœ… ä¸»é¡ŒåµŒå…¥å‘é‡å·²ä¿å­˜åˆ°: {topics_path}")

        # index.json æ›´æ–°
        file_service.update_file_status(subject, "lectures", original_filename, "done")

        logger.info(f"âœ… è¬›ç¾©{filename_without_ext}è™•ç†å®Œæˆ")
        return {
            "success": True, 
            "message": "è¬›ç¾©è™•ç†å®Œæˆ", 
            "output_path": path, 
            "keypoints_path": keypoints_path,
            "tree_path": tree_path
        }
    except Exception as e:
        try:
            # index.json æ›´æ–°
            file_service.update_file_status(subject, "lectures", original_filename, "error")
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° index.json æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        
        logger.error(f"âŒ è™•ç†è¬›ç¾©æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        return {"success": False, "error": str(e)}


def _extract_keypoints_hierarchy(chapter: dict):
    """
    å¾å·¢ç‹€è¬›ç¾© JSON æª”ä¸­æå–æ‰€æœ‰ keypointï¼Œä¸¦æ¨™è¨»å…¶æ‰€å±¬çš„ç« ç¯€ / ä¸»é¡Œ / é é¢ç´¢å¼•ã€‚
    å°‡çµæœå„²å­˜æˆæ–°çš„ JSON æª”æ¡ˆã€‚
    """
    try:
        keypoints_list = []

        if not chapter or not isinstance(chapter, dict):
            logger.error("ç« ç¯€è³‡æ–™ç„¡æ•ˆæˆ–ç‚ºç©º")
            return keypoints_list

        sections = chapter.get('Sections', [])
        if not sections:
            logger.warning("ç« ç¯€ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ®µè½")
            return keypoints_list

        for s_idx, section in enumerate(sections):
            if not section or not isinstance(section, dict):
                logger.warning(f"ç¬¬ {s_idx+1} å€‹æ®µè½è³‡æ–™ç„¡æ•ˆï¼Œå·²è·³é")
                continue

            topics = section.get('Topics', [])
            if not topics:
                logger.warning(f"ç¬¬ {s_idx+1} å€‹æ®µè½ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•ä¸»é¡Œ")
                continue

            for t_idx, topic in enumerate(topics):
                if not topic or not isinstance(topic, dict):
                    logger.warning(f"ç¬¬ {s_idx+1} æ®µè½ä¸­ç¬¬ {t_idx+1} å€‹ä¸»é¡Œè³‡æ–™ç„¡æ•ˆï¼Œå·²è·³é")
                    continue

                pages = topic.get('Pages', [])
                if not pages:
                    logger.warning(f"ç¬¬ {s_idx+1} æ®µè½çš„ç¬¬ {t_idx+1} å€‹ä¸»é¡Œä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•é é¢")
                    continue

                for p_idx, page in enumerate(pages):
                    if not page or not isinstance(page, dict):
                        logger.warning(f"ç¬¬ {s_idx+1} æ®µè½ç¬¬ {t_idx+1} ä¸»é¡Œä¸­ç¬¬ {p_idx+1} é è³‡æ–™ç„¡æ•ˆï¼Œå·²è·³é")
                        continue

                    keypoints = page.get('Keypoints', [])
                    if not keypoints:
                        # é€™æ˜¯æ­£å¸¸çš„ï¼Œå› ç‚ºä¸æ˜¯æ¯å€‹é é¢éƒ½æœ‰é—œéµé»
                        continue

                    page_idx = page.get("page_idx", p_idx + 1)

                    for k_idx, k in enumerate(keypoints):
                        if not k or not isinstance(k, dict):
                            logger.warning(f"åœ¨ç¬¬ {page_idx} é ä¸­æ‰¾åˆ°ç„¡æ•ˆé—œéµé»ï¼Œå·²è·³é")
                            continue

                        keypoint = {
                            "Title": k.get("Title", "").strip(),
                            "Content": k.get("Content", "").strip(),
                            "Index": [s_idx, t_idx, p_idx, k_idx],
                            "from_page": page_idx
                        }

                        # åªæ·»åŠ æœ‰å…§å®¹çš„é—œéµé»
                        if keypoint["Title"] or keypoint["Content"]:
                            keypoints_list.append(keypoint)

        logger.info(f"å·²å¾ç« ç¯€çµæ§‹ä¸­æå– {len(keypoints_list)} å€‹é‡é»")
        return keypoints_list
        
    except Exception as e:
        logger.error(f"æå–é‡é»å±¤æ¬¡çµæ§‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
        return []


def _extract_topics_hierarchy(chapter: dict):
    """
    å¾å·¢ç‹€è¬›ç¾© JSON æª”ä¸­æå–æ‰€æœ‰ topicsï¼Œä¸¦æ¨™è¨»å…¶æ‰€å±¬çš„ç« ç¯€ / ä¸»é¡Œ / é é¢ç´¢å¼•ã€‚
    å°‡çµæœå„²å­˜æˆæ–°çš„ JSON æª”æ¡ˆã€‚
    """
    topics_list = []
    k_count = 0
    for section in chapter["Sections"]:
        for topic in section["Topics"]:
            topic_info = {
                "Topic": topic["Topic"],
                "k_start": k_count,
            }

            for page in topic["Pages"]:
                for keypoint in page["Keypoints"]:
                    keypoint["Topic"] = topic["Topic"]
                    k_count += 1

            topic_info["k_end"] = k_count
            topic_info["Wrong_count"] = 0
            #topic_info["Correct_count"] = 0
            topics_list.append(topic_info)
    
    return topics_list


def processing_get_keypoints(subject, lecture_name):
    # è®€å– json
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    # print(keypoints_path)
    keypoints_json = text.read_json(keypoints_path, default_content=[])

    for keypoint in keypoints_json:
        # åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½ï¼ˆå®‰å…¨ popï¼‰
        for field in ["Index", "from_page", "Embedding"]:
            keypoint.pop(field, None)

        # è™•ç† Notes
        notes = keypoint.pop("Notes", None)
        if notes is not None:
            keypoint["Notes_count"] = len(notes)

        # é‡å‘½åé€²åº¦æ¬„ä½
        if "Learning_Progress" in keypoint:
            keypoint["Progress"] = keypoint.pop("Learning_Progress")

    return keypoints_json

'''
def processing_get_notes(subject, note_name):
    """
    ç²å–æŒ‡å®šç­†è¨˜çš„è©³ç´°ä¿¡æ¯
    
    Args:
        subject: ç§‘ç›®åç¨±
        note_name: ç­†è¨˜æ–‡ä»¶å
    
    Returns:
        dict: åŒ…å«ç­†è¨˜è©³ç´°ä¿¡æ¯çš„å­—å…¸ï¼Œæ ¼å¼ç‚º {"Lecture_Name": str, "Notes": list}
    """
    # è®€å– json
    note_name_without_ext = os.path.splitext(note_name)[0]
    note_path = os.path.join("app", "data_server", subject, "notes", note_name_without_ext + ".json")
    
    try:
        note_json = text.read_json(note_path, default_content={})
        
        # åˆ¤æ–·è¿”å›çš„æ•¸æ“šæ ¼å¼ï¼Œé€²è¡Œé©ç•¶è™•ç†
        if isinstance(note_json, dict) and "Notes" in note_json and "Lecture_Name" in note_json:
            # æ ¼å¼å·²ç¶“æ˜¯ {"Lecture_Name": xxx, "Notes": [...]} 
            result = note_json
        elif isinstance(note_json, list):
            # å¦‚æœåªæ˜¯ç­†è¨˜åˆ—è¡¨ï¼Œå‰‡åŒ…è£æˆé æœŸæ ¼å¼
            result = {
                "Lecture_Name": note_name,  # ç”¨ç­†è¨˜åç¨±ä½œç‚ºè¬›ç¾©åç¨±çš„é è¨­å€¼
                "Notes": note_json
            }
        else:
            # å…¶ä»–æ ¼å¼æƒ…æ³ï¼Œæ§‹å»ºä¸€å€‹é è¨­çµæ§‹
            result = {
                "Lecture_Name": note_name,
                "Notes": [note_json] if note_json else []
            }
        
        # è™•ç† Notes æ•¸çµ„ä¸­æ¯å€‹ç­†è¨˜
        for note in result.get("Notes", []):
            # åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½ï¼ˆå®‰å…¨ popï¼‰
            for field in ["Embedding", "Keypoint_id"]:
                note.pop(field, None)
            

        
        logger.info(f"æˆåŠŸç²å–ç­†è¨˜ '{note_name}' çš„è©³ç´°ä¿¡æ¯ï¼Œå…± {len(result.get('Notes', []))} æ¢ç­†è¨˜")
        return result
        
    except Exception as e:
        logger.error(f"ç²å–ç­†è¨˜ '{note_name}' è©³ç´°ä¿¡æ¯å¤±æ•—: {str(e)}")
        # è¿”å›ä¸€å€‹ç©ºçš„çµæ§‹ä»¥ä¿æŒæ ¼å¼ä¸€è‡´
        return {
            "Lecture_Name": note_name,
            "Notes": []
        }
'''

def processing_get_notes(subject, lecture_name):
    try:
        related_notes = []
        
        note_index_path = os.path.join("app", "data_upload", subject, "notes", "index.json")
        note_index_json = text.read_json(note_index_path, default_content=[])

        for note_entry in note_index_json:
            note_name_without_ext = os.path.splitext(note_entry["filename"])[0]
            note_path = os.path.join("app", "data_server", subject, "notes", note_name_without_ext + ".json")
            note_json = text.read_json(note_path, default_content=[])

            print(f"Compare: {note_json['Lecture_Name']} == {lecture_name}")
            if note_json["Lecture_Name"] == lecture_name:
                print(f"æ‰¾åˆ°è¬›ç¾© {lecture_name} çš„ç­†è¨˜: {note_entry['filename']}")
                for note in note_json.get("Notes", []):
                    # åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½ï¼ˆå®‰å…¨ popï¼‰
                    for field in ["Embedding"]:
                        note.pop(field, None)
                    
                    # note["Lecture_Name"] = note_json["Lecture_Name"]
                    related_notes.append(note)

        return related_notes
    except Exception as e:
        logger.error(f"ç²å–ç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

def processing_get_questions(subject, lecture_name, num_questions):
    # è®€å– json
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    keypoints_json = text.read_json(keypoints_path, default_content=[])

    if not keypoints_json:
        raise ValueError("ç„¡æ³•è®€å–è¬›ç¾©é‡é»ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”éç©º")
    
    QM = QuestioningManager(subject, keypoints_json)

    questions = []
    for i in range(num_questions):
        questions.append(QM.get_question())

    return questions


def processing_update_weights(subject, lecture_name, answer_results):
    # è®€å– json
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    keypoints_json = text.read_json(keypoints_path, default_content=[])

    if not keypoints_json:
        raise ValueError("ç„¡æ³•è®€å–è¬›ç¾©é‡é»ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”éç©º")
    
    QM = QuestioningManager(subject, keypoints_json)

    # æ›´æ–°æ¬Šé‡
    edited_keypoints, overall_lr = QM.update_weights(answer_results)
    text.write_json(edited_keypoints, keypoints_path)

    # æ›´æ–°ç¸½å­¸ç¿’ç‡æ­·å²
    overall_lr_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_overall_lr.json")
    overall_lr_json = text.read_json(overall_lr_path,default_content=[])
    overall_lr_json.append(overall_lr)
    text.write_json(overall_lr_json, overall_lr_path)

    return {
        "success": True,
        "message": "æ›´æ–°æ¬Šé‡å®Œæˆ",
    }

def processing_update_topics(subject, lecture_name, answer_results):
    # è®€å– json
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    topics_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_topics.json")
    topics_json = text.read_json(topics_path, default_content=[])

    # åˆå§‹åŒ–ä¸»é¡ŒéŒ¯é¡Œæ•¸
    for topic in topics_json:
        topic["Wrong_count"] = 0

    # æ›´æ–°ä¸»é¡ŒéŒ¯é¡Œæ•¸
    for result in answer_results:
        k_idx = result["Keypoints_Index"]
        is_Correct = result["is_Correct"]
        
        if not is_Correct:
            # å°‹æ‰¾k_idxå°æ‡‰çš„topic
            for topic in topics_json:
                if k_idx >= topic["k_start"] and k_idx < topic["k_end"]:
                    topic["Wrong_count"] += 1
                    break

    text.write_json(topics_json, topics_path)

    return {
        "success": True,
        "message": "æ›´æ–°ä¸»é¡Œå®Œæˆ",
    }


def processing_get_page_info(subject, lecture_name, page_index):
    lecturename_without_ext = os.path.splitext(lecture_name)[0]


    # 1. page image
    result_img_base64 = None
    imgs_dir_path = os.path.join("app", "data_upload", subject, "lectures", lecturename_without_ext + "_imgs")
    if not os.path.exists(imgs_dir_path): # å¦‚æœæ²’æœ‰åœ–ç‰‡ï¼Œå‰‡å¾pdfè½‰æ›
        
        pdf_path = os.path.join("app", "data_upload", subject, "lectures", lecture_name)
        imgs = media.read_pdf_to_images(pdf_path)
        result_img = imgs[page_index]
        # å°‡PILåœ–åƒè½‰æ›ç‚ºbase64å­—ä¸²
        result_img_base64 = media.convert_PIL_to_base64(result_img)
    else: # å¦‚æœåœ–ç‰‡å­˜åœ¨ï¼Œå‰‡å¾åœ–ç‰‡ä¸­è®€å–
        img_path = os.path.join(imgs_dir_path, f"{lecturename_without_ext}_{page_index}.png")
        result_img = media.read_image_to_PIL(img_path)
        result_img_base64 = media.convert_PIL_to_base64(result_img)

    # è®€å– json
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    keypoints_json = text.read_json(keypoints_path, default_content=[])
    target_keypoints = list(filter(lambda x: x["from_page"] == page_index, keypoints_json))

    # 2. keypoints
    result_keypoints = []
    for keypoint in target_keypoints:
        temp = {"Title": keypoint["Title"], "Content": keypoint["Content"]}
        result_keypoints.append(temp)

    # 3. notes
    result_notes = []
    for keypoint in target_keypoints:
        if "Notes" in keypoint:
            notes = file_service.get_notes_from_keypoint(subject, keypoint)
            for note in notes:
                temp = {"Title": note["Title"], "Content": note["Content"]}
                result_notes.append(temp)

    result = {"Image": result_img_base64,
              "Keypoints": result_keypoints,
              "Notes": result_notes}
    return result


def processing_get_history(subject, lecture_name):
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    keypoints_json = text.read_json(keypoints_path, default_content=[])

    topics_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_topics.json")
    topics_json = text.read_json(topics_path, default_content=[])
    print('topics_json:', topics_json)

    logger.info('è™•ç†æ­·å²æ•¸æ“šä¸­...')
    # 1. ä¸»é¡Œå­¸ç¿’ç‡
    result_t_lrs = []
    for topic in topics_json:
        if topic["k_end"] - topic["k_start"] == 0:
            continue

        t_lr = 0
        t_weight = 0
        for k_idx in range(topic["k_start"], topic["k_end"]):
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦åˆæ³•
            if k_idx >= len(keypoints_json):
                logger.warning(f"ä¸»é¡Œ '{topic['Topic']}' çš„ç´¢å¼•è¶…å‡ºç¯„åœ: {k_idx} >= {len(keypoints_json)}")
                continue
                
            kp = keypoints_json[k_idx]
            # æª¢æŸ¥ Learning_Rate æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨ 0.0 ä½œç‚ºé»˜èªå€¼
            if "Learning_Rate" not in kp:
                logger.warning(f"é—œéµé» {k_idx} ä¸­ç¼ºå°‘ 'Learning_Rate' å±¬æ€§ï¼Œä½¿ç”¨é»˜èªå€¼ 0.0")
                t_lr += 0.0
            else:
                t_lr += kp["Learning_Rate"] * kp["Difficulty"] * 3
                t_weight += kp["Difficulty"] * 3

        # é¿å…é™¤ä»¥é›¶
        if t_weight > 0:
            t_lr /= t_weight
        else:
            t_lr = 0.0
            
        temp = {"Topic": topic["Topic"], "Learning_Rate": t_lr}
        result_t_lrs.append(temp)

    logger.info('ä¸»é¡Œå­¸ç¿’ç‡è™•ç†å®Œæˆ')
    # 2. ä¸»é¡ŒéŒ¯é¡Œæ•¸
    result_t_wrong_count = []
    for topic in topics_json:
        if "Wrong_count" not in topic or topic["Wrong_count"] == 0:
            continue

        temp = {"Topic": topic["Topic"], "Wrong_count": topic["Wrong_count"]}
        result_t_wrong_count.append(temp)

    logger.info('ä¸»é¡ŒéŒ¯é¡Œæ•¸è™•ç†å®Œæˆ')
    # 3. ç¸½å­¸ç¿’ç‡æ­·å²
    overall_lr_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_overall_lr.json")
    overall_lr_json = text.read_json(overall_lr_path,default_content=[])

    logger.info('ç¸½å­¸ç¿’ç‡æ­·å²è™•ç†å®Œæˆ')
    result = {
        "t_lrs": result_t_lrs,
        "t_wrong_count": result_t_wrong_count,
        "overall_lr_history": overall_lr_json
    }   

    logger.info(f'æ­·å²æ•¸æ“šè™•ç†å®Œæˆ: \n{result}')
    return result


def processing_query_keypoint(subject, lecture_name, query_text):
    lecturename_without_ext = os.path.splitext(lecture_name)[0]
    keypoints_path = os.path.join("app", "data_server", subject, "lectures", lecturename_without_ext + "_keypoints.json")
    keypoints_json = text.read_json(keypoints_path, default_content=[])

    OpenAI = OpenAIRequest()
    query_embedding = OpenAI.processing_embedding([query_text])
    query_embedding = query_embedding[0]

    # è¨ˆç®—æ¯å€‹é‡é»çš„é¤˜å¼¦ç›¸ä¼¼åº¦
    keypoints_embedding = [keypoint["Embedding"] for keypoint in keypoints_json]
    try:
        target_k_idx = sim.get_most_similar_index(query_embedding, keypoints_embedding,threshold=0.30, need_threshold=True)
    except Exception as e:
        logger.error(f"æŸ¥è©¢é‡é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


    if target_k_idx == -1:
        return {
            "Title": "æ‰¾ä¸åˆ°ç›¸é—œé‡é»",
            "Content": "æ‰¾ä¸åˆ°ç›¸é—œé‡é»",
            "Explanation": "æ‰¾ä¸åˆ°ç›¸é—œé‡é»"
        }

    target_keypoint = keypoints_json[target_k_idx]

    # ç”Ÿæˆé‡é»è§£é‡‹
    explanation = OpenAI.processing_keypoint_explanation(subject, target_keypoint)

    result = {
        "Title": target_keypoint["Title"],
        "Content": target_keypoint["Content"],
        "Explanation": explanation["Explanation"]
    }

    return result


